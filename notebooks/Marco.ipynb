{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bdab86a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from diffusers import AutoencoderKL\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "69c284a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PARAMETRI DI CONFIGURAZIONE GLOBALI ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Questi valori sono per un training leggero (adatta al tuo hardware)\n",
    "LATENT_CHANNELS = 4 # Output VAE: 4 canali\n",
    "FEATURES = [128, 256, 512]\n",
    "TIMESTEPS = 1000\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4 \n",
    "EPOCHS = 5 # Solo per un test iniziale\n",
    "SAVE_FOLDER = \"../weights/\"\n",
    "MODEL_NAME = \"ldm_unet\"\n",
    "DATA_FOLDER = \"../data/val2017\"\n",
    "IMAGE_SIZE = 128\n",
    "VALIDATION_SPLIT_RATIO = 0.2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e852711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. VAE ENCODING & DECODING UTILITY ---\n",
    "# Caricamento del VAE e definizione delle utility (come discusso in precedenza)\n",
    "try:\n",
    "    vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\").to(DEVICE).eval()\n",
    "    VAE_SCALE_FACTOR = vae.config.scaling_factor\n",
    "except Exception as e:\n",
    "    print(f\"Errore nel caricamento del VAE. Assicurati di avere 'diffusers' installato. {e}\")\n",
    "    VAE_SCALE_FACTOR = 0.18215 # Valore fallback\n",
    "\n",
    "def encode_to_latent(pixels: torch.Tensor):\n",
    "    \"\"\"Converte pixel [0, 1] in latenti (4, H/8, W/8).\"\"\"\n",
    "    pixels = (pixels * 2) - 1.0 # Scala [0, 1] a [-1, 1]\n",
    "    with torch.no_grad():\n",
    "        posterior = vae.encode(pixels).latent_dist\n",
    "    latents = posterior.sample()\n",
    "    return latents * VAE_SCALE_FACTOR\n",
    "\n",
    "def decode_from_latent(latents: torch.Tensor):\n",
    "    \"\"\"Converte latenti in pixel [0, 1].\"\"\"\n",
    "    latents = latents / VAE_SCALE_FACTOR\n",
    "    with torch.no_grad():\n",
    "        image = vae.decode(latents).sample\n",
    "    return (image / 2 + 0.5).clamp(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a83c8103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. CLASSI ESSENZIALI (Definizione Minimali) ---\n",
    "# Necessario inserire qui il codice corretto di ResBlock e UNet per far funzionare lo script!\n",
    "# Ho usato il codice corretto per garantire l'esecuzione.\n",
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        \n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        \n",
    "        # 'embeddings' will be (B, half_dim)\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :] \n",
    "        # broadcasting (B, half_dim)\n",
    "        \n",
    "        # Sine and Cosine\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1) # (B, dim)\n",
    "        \n",
    "        # if dim is odd, pad with one zero vector\n",
    "        if self.dim % 2 != 0:\n",
    "            embeddings = F.pad(embeddings, (0, 1), mode='constant', value=0)\n",
    "            \n",
    "        return embeddings\n",
    "    \n",
    "\n",
    "\n",
    "# Residual Block\n",
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Residual Block with two convolutional layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, time_embed_dim = 128, num_groups=8, self_attention=False):\n",
    "        super(ResBlock, self).__init__()\n",
    "\n",
    "        # Time Embedding\n",
    "        self.time_proj1 = nn.Linear(time_embed_dim, channels * 2)\n",
    "        self.time_proj2 = nn.Linear(time_embed_dim, channels * 2)\n",
    "\n",
    "        # Input Number of channels (128, 256, 512) x h (16, 32) x w (16, 32)\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.norm1 = nn.GroupNorm(num_groups=num_groups, num_channels=channels)\n",
    "        self.act = nn.SiLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.norm2 = nn.GroupNorm(num_groups = num_groups, num_channels = channels)\n",
    "        \n",
    "        # Output channels = Input channels \n",
    "\n",
    "        # Self-attention layer (optional)\n",
    "        if self_attention:\n",
    "            self.attention = nn.MultiheadAttention(embed_dim=channels, num_heads=4)\n",
    "\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        identity = x\n",
    "        # Adaptive Normalization with Time Embedding\n",
    "\n",
    "        # Time embedding 1 projection \n",
    "        t_proj_1 = self.time_proj1(t_emb).chunk(2, dim=-1)\n",
    "        gamma1, beta1 = t_proj_1[0].unsqueeze(-1).unsqueeze(-1), t_proj_1[1].unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.norm1(out) * (1 + gamma1) + beta1\n",
    "        out = self.act(out)\n",
    "\n",
    "        # Time embedding 2 projection \n",
    "        t_proj_2 = self.time_proj2(t_emb).chunk(2, dim=-1)\n",
    "        gamma2, beta2 = t_proj_2[0].unsqueeze(-1).unsqueeze(-1), t_proj_2[1].unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.norm2(out) * (1 + gamma2) + beta2\n",
    "        out = self.act(out)\n",
    "\n",
    "     \n",
    "        \n",
    "        if hasattr(self, \"attention\"):\n",
    "            b, c, h, w = out.size()\n",
    "            out_reshaped = out.view(b, c, h * w).permute(2, 0, 1)  # (h*w, b, c)\n",
    "            out_attended, _ = self.attention(out_reshaped, out_reshaped, out_reshaped)\n",
    "            out = out_attended.permute(1, 2, 0).view(b, c, h, w)\n",
    "\n",
    "        out += identity # Residual connection\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# UNet Model\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    LDM UNet model skeleton.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, num_blocks = 2, time_emb_dim = 128, features=[128, 256, 512]):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "\n",
    "\n",
    "        #The Variational autoencoder reduces the input image of size 3x128x128 to a latent representation of size 4 x 32 x 32.\n",
    "        # Input\n",
    "        # 4 x 32 x 32\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.time_proj = SinusoidalPositionEmbeddings(dim=time_emb_dim)\n",
    "\n",
    "        # Time Embedding MLP\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(time_emb_dim, time_emb_dim * 4), \n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim * 4, time_emb_dim) \n",
    "        )\n",
    "        self.time_emb_dim = time_emb_dim\n",
    "\n",
    "\n",
    "\n",
    "        # Initial Convolution out = [H_in + 2*padding - dilation*(kernel_size-1) -1]/stride +1\n",
    "        # Output\n",
    "        # 128 x 32 x 32\n",
    "        self.init_conv = nn.Conv2d(in_channels, features[0], kernel_size=3, padding=1)\n",
    "\n",
    "        # Encoder\n",
    "        self.enc_layers = nn.ModuleList()\n",
    "        self.downsamples = nn.ModuleList()\n",
    "\n",
    "\n",
    "        current_channels = features[0]\n",
    "\n",
    "        for next_channels in features[1:]:\n",
    "            level_blocks = nn.ModuleList()\n",
    "            for _ in range(num_blocks):\n",
    "                block = ResBlock(current_channels, time_embed_dim=time_emb_dim, num_groups = min(current_channels//32, 32))\n",
    "                level_blocks.append(block)\n",
    "\n",
    "            # Output size halved (DownSampling Layer)\n",
    "            downsample = nn.Conv2d(current_channels, next_channels, kernel_size=4, stride=2, padding=1)\n",
    "            self.downsamples.append(downsample)\n",
    "            self.enc_layers.append(level_blocks)\n",
    "            current_channels = next_channels\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.ModuleList([\n",
    "        ResBlock(features[-1], time_embed_dim=time_emb_dim, num_groups = min(features[-1]//32, 32), self_attention=True),\n",
    "        ResBlock(features[-1], time_embed_dim=time_emb_dim, num_groups = min(features[-1]//32, 32), self_attention=True)])\n",
    "\n",
    "\n",
    "        # Decoder\n",
    "        self.dec_layers = nn.ModuleList()\n",
    "        self.upsamples = nn.ModuleList()\n",
    "        reversed_features = features[::-1]\n",
    "        \n",
    "        # Modified to Allow Skip Connections\n",
    "        for i in range(len(reversed_features) - 1):\n",
    "            level_blocks = nn.ModuleList()\n",
    "            \n",
    "            out_channels_level = reversed_features[i+1]\n",
    "\n",
    "            in_channels_up = reversed_features[i]\n",
    "\n",
    "            # UpSampling: Reduces channels while doubling spatial size.\n",
    "            upsample = nn.ConvTranspose2d(in_channels_up, out_channels_level, kernel_size=4, stride=2, padding=1)\n",
    "            self.upsamples.append(upsample)\n",
    "            \n",
    "            # After UpSampling + Skip Connection, the channel count will be N_current * 2\n",
    "            block_in_channels_after_skip = out_channels_level * 2\n",
    "\n",
    "            \n",
    "            # To solve the channel mismatch after concatenation with skip connections,\n",
    "            # we introduce an adapter convolutional layer.\n",
    "            adapter_conv = nn.Conv2d(block_in_channels_after_skip, out_channels_level, kernel_size=1)\n",
    "            level_blocks.append(adapter_conv) # Adapter is the first operation in this decoder level\n",
    "\n",
    "            for _ in range(num_blocks):\n",
    "\n",
    "                block = ResBlock(out_channels_level, time_embed_dim=time_emb_dim, num_groups = min(out_channels_level//32, 32))\n",
    "                level_blocks.append(block)\n",
    "            \n",
    "            self.dec_layers.append(level_blocks)\n",
    "\n",
    "\n",
    "        in_channels = features[0]\n",
    "        self.out_conv = nn.Sequential(\n",
    "        nn.GroupNorm(num_groups= min(in_channels//32, 32), num_channels=in_channels),\n",
    "        nn.SiLU(),\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x, time):\n",
    "\n",
    "        time_sin = self.time_proj(time)         \n",
    "        t_emb = self.time_mlp(time_sin)\n",
    "        x = self.init_conv(x)\n",
    "\n",
    "        # skip connections\n",
    "        skips = []\n",
    "\n",
    "        # encoder\n",
    "        for blocks, down in zip(self.enc_layers, self.downsamples):\n",
    "            for blk in blocks:\n",
    "                x = blk(x, t_emb)\n",
    "            skips.append(x)\n",
    "            x = down(x)\n",
    "        \n",
    "        # skip = [Encoder 1 --> 128 x 32 x 32 , Encoder 2 --> 256 x 16 x 16]\n",
    "        # bottleneck\n",
    "        for layer in self.bottleneck:\n",
    "            x = layer(x, t_emb)\n",
    "\n",
    "        # decoder for skip connections\n",
    "        for up, blocks, skip in zip(self.upsamples, self.dec_layers, reversed(skips)):\n",
    "            x = up(x)\n",
    "            # # # if shapes mismatch due to odd sizes, center-crop skip\n",
    "            if x.shape[-2:] != skip.shape[-2:]:\n",
    "                    # simple interpolate to match\n",
    "                    #_, _, h, w = x.shape\n",
    "                x = F.interpolate(x, size=skip.shape[-2:], mode='nearest')\n",
    "            \n",
    "            #concat along channels\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "            for blk in blocks:\n",
    "                if isinstance(blk, nn.Conv2d):\n",
    "                    # Adapter conv\n",
    "                    x = blk(x)\n",
    "                else:\n",
    "                    x = blk(x, t_emb)\n",
    "\n",
    "        # final conv\n",
    "        out = self.out_conv(x)\n",
    "\n",
    "\n",
    "        return out\n",
    "\n",
    "class LatentDataset(torch.utils.data.Dataset):\n",
    "    # ... Inserire qui il codice della classe LatentDataset fornito\n",
    "    def __init__(self, data_dir, image_size=128):\n",
    "        self.data_dir = data_dir\n",
    "        self.image_paths = [\n",
    "            os.path.join(data_dir, f) \n",
    "            for f in os.listdir(data_dir) \n",
    "            if f.endswith(('.png', '.jpg', '.jpeg'))\n",
    "        ]\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(image_size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(image_size),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except:\n",
    "            # Semplice fallback per dataset di test\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "            \n",
    "        return self.transform(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90a02165",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardConfig:\n",
    "    \"\"\"\n",
    "    Parameters configuartion for the forward process\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_path=\"latents.pt\",\n",
    "        output_path=\"latents_noised.pt\",\n",
    "        t: float = 0.7,\n",
    "        final: bool = True,\n",
    "        eps: float = 1e-5,\n",
    "        closed_formula: bool = True,\n",
    "        seed: int = 42,\n",
    "        beta_min: float = 0.1,\n",
    "        beta_max: float = 20.0,\n",
    "        N: int = 1000,\n",
    "    ):\n",
    "        self.input_path = input_path\n",
    "        self.output_path = output_path\n",
    "        self.t = t\n",
    "        self.final = final\n",
    "        self.eps = eps\n",
    "        self.closed_formula = closed_formula\n",
    "        self.seed = seed\n",
    "        self.beta_min = beta_min\n",
    "        self.beta_max = beta_max\n",
    "        self.N = N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e64e4154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class subVP_SDE:\n",
    "    def __init__(self, beta_min=0.1, beta_max=20, N=1000):\n",
    "        \"\"\"Construct the sub-VP SDE\n",
    "\n",
    "        Args:\n",
    "        beta_min: value of beta(0)\n",
    "        beta_max: value of beta(1)\n",
    "        N: number of discretization steps\n",
    "\n",
    "        Attributes:\n",
    "        beta_0: minimum noise scale at t=0 for the linear schedule.\n",
    "        beta_1: maximum noise scale at t=1 for the linear schedule.\n",
    "        N: stored grid size, which is usually not used by closed-form routines below.\n",
    "        \"\"\"\n",
    "        self.beta_0 = beta_min\n",
    "        self.beta_1 = beta_max\n",
    "        self.N = N\n",
    "\n",
    "    def beta_linear(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Return β(t) = β_0 + t*(β_1 - β_0)\n",
    "        Used for closed form forward application\n",
    "        \"\"\"\n",
    "        return self.beta_0 + t * (self.beta_1 - self.beta_0)\n",
    "\n",
    "    def beta_exponential(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"β(t) grows exponentially from β_0 to β_1 across t in [0, 1]\"\"\"\n",
    "        sequence = torch.log(torch.tensor(self.beta_1/self.beta_0, device = t.device, dtype = t.dtype))\n",
    "        return self.beta_0 * torch.exp(sequence * t)\n",
    "\n",
    "    # Instanteneous SDE coefficients\n",
    "    def sde(self, x, t):\n",
    "        \"\"\"Returns instantaneous coefficients of the SDE evaluated at (x,t).\n",
    "        This function do not integrate but it provides the per-time drift and diffusion values.\n",
    "\n",
    "        Args:\n",
    "        x: (B,C,H,W), t: (B,)\n",
    "        \n",
    "        Details:\n",
    "        beta(t) = beta_0 + t * (beta_1 - beta_0)\n",
    "        ∫_0^t beta(s) ds = beta_0 * t + 0.5 * (beta_1 - beta_0) * t^2\n",
    "        discount := 1 - exp(-2 * ∫_0^t beta(s) ds) = 1 - exp(-2 * beta_0 * t - (beta_1 - beta_0) * t^2)\n",
    "        g(t) = sqrt(beta(t) * discount)\n",
    "        \"\"\"\n",
    "        beta_t = self.beta_0 + t * (self.beta_1 - self.beta_0)\n",
    "        drift = -0.5 * beta_t[:, None, None, None] * x #because x is (B, C, H, W), where B is the batch size\n",
    "        discount = 1.0 - torch.exp(-2 * self.beta_0 * t - (self.beta_1 - self.beta_0) * t ** 2) #development of the integral beta(s) ds\n",
    "        diffusion = torch.sqrt(beta_t * discount)\n",
    "        return drift, diffusion\n",
    "\n",
    "    # Closed form formula\n",
    "    def marginal_prob(self, x_0, t):\n",
    "        \"\"\"Closed form for X_t given X_0 = x_O\n",
    "        \n",
    "        Distribution:\n",
    "        X_t|X_0 ~ N (mean_coeff*X_0, std^2 * I)\n",
    "\n",
    "        Args:\n",
    "        x0: (B,C,H,W), t: (B,)\n",
    "        Returns mean (B,1,1,1)*x0 and std (B,)\n",
    "\n",
    "        Details:\n",
    "        ∫_0^t beta(s) ds = beta_0 * t + 0.5 * (beta_1 - beta_0) * t^2\n",
    "        log_mean_coeff = -0.5 * ∫_0^t beta(s) ds\n",
    "                        = -0.5 * beta_0 * t - 0.25 * (beta_1 - beta_0) * t^2\n",
    "        mean = exp(log_mean_coeff) * x_0\n",
    "        std  = sqrt(1 - exp(2 * log_mean_coeff))\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        log_mean_coeff = -0.5 * self.beta_0 * t + -0.25 * (t ** 2) * (self.beta_1 - self.beta_0) #log exp(-1/2 * integral ( beta(s) ds)\n",
    "        mean = torch.exp(log_mean_coeff)[:, None, None, None] * x_0\n",
    "        std = torch.sqrt(1.0 - torch.exp(2 * log_mean_coeff)) #double check\n",
    "        return mean, std\n",
    "\n",
    "    def perturb_closed(self, x_0, t, noise = None):\n",
    "        \"\"\"Sample X_t by perturbing X_0 with gaussian noise\n",
    "\n",
    "        Operation:\n",
    "        1. Compute closed-form mean and std of X_t | X_0.\n",
    "        2. Draw epsilon ~ N(0, I) with the same shape as x_0 if not provided.\n",
    "        3. Return x_t = mean + std * epsilon, along with epsilon and std.\n",
    "\n",
    "         Notes:\n",
    "          - Deterministic for fixed x_0, t, and noise.\n",
    "          - Suitable for training score/ε-predictor networks with known std.\n",
    "          \n",
    "        Args:\n",
    "        x_0: (B,C,H,W), t:(B,)\"\"\"\n",
    "        \n",
    "        mean, std = self.marginal_prob(x_0, t)\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_0)\n",
    "        x_t = mean + std[:,None, None, None] * noise\n",
    "        return x_t, noise, std\n",
    "\n",
    "    def perturb_simulate_path(self, x_0: torch.Tensor, t_end: float = 0.5, steps: int = 500, seed: int = 42, eps: float = 1e-12):\n",
    "        \"\"\"Sample X_t by perturbing X_0 with gaussian noise at time t\n",
    "\n",
    "        Operation:\n",
    "        1. Compute simulate path for of X_t | X_t-1 and updating X_t values for steps time\n",
    "        2. omputing the mean and std at time t\n",
    "        3. Calculating the implied eps\n",
    "\n",
    "         Notes:\n",
    "          - Deterministic for fixed x_0, t, and noise.\n",
    "          - Suitable for training score/ε-predictor networks with known std.\n",
    "          \n",
    "        Args:\n",
    "        x_0: (B,C,H,W), t:(B,)\"\"\"\n",
    "        t_scalar = float(t_end)\n",
    "        \n",
    "        device = x_0.device\n",
    "        dtype = x_0.dtype\n",
    "        B = x_0.shape[0]\n",
    "    \n",
    "        gen = torch.Generator(device = device)\n",
    "        gen.manual_seed(seed)\n",
    "\n",
    "        t_grid = torch.linspace(0.0, t_scalar, steps+1, device = device, dtype = dtype)\n",
    "        x = x_0.clone()\n",
    "        \n",
    "        for k in range(steps):\n",
    "            t_k = t_grid[k].expand(B)\n",
    "            dt = (t_grid[k+1] - t_grid[k]).item() # we return a scalar value\n",
    "            drift, diffusion = self.sde(x, t_k)\n",
    "            noise = noise = torch.randn(x.shape, device=x.device, dtype=x.dtype, generator=gen) # we generate Gaussian Noise, with same device and dtype as x\n",
    "            x = x + drift * dt + diffusion[:, None, None, None] * (dt ** 0.5) * noise\n",
    "        \n",
    "        t_tensor = torch.full((B,), t_scalar, device = device, dtype = dtype)\n",
    "        mean_t, std_t = self.marginal_prob(x_0, t_tensor)\n",
    "        eps_implied = (x - mean_t) / (std_t[:, None, None, None] +1e-12) #noise tensor\n",
    "        return x, eps_implied, std_t\n",
    "    \n",
    "    def get_integral_beta(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Computes the integral of beta(s) from 0 to t.\"\"\"\n",
    "        # ∫_0^t beta(s) ds = beta_0 * t + 0.5 * (beta_1 - beta_0) * t^2\n",
    "        return self.beta_0 * t + 0.5 * (self.beta_1 - self.beta_0) * t ** 2\n",
    "    \n",
    "            \n",
    "    def get_g_squared(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the coefficient g(t)^2.\n",
    "        \n",
    "        λ(t) is used in the SDE definition as the diffusion coefficient squared.\n",
    "        λ(t) = g(t)^2 = β(t) * (1 - exp(-2 * ∫_0^t β(s) ds))\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # 1. β(t)\n",
    "        beta_t = self.beta_linear(t)\n",
    "        \n",
    "        # 2. ∫_0^t β(s) ds\n",
    "        integral_beta = self.get_integral_beta(t)\n",
    "        \n",
    "        # 3. 'discount' factor (1 - exp(-2 * Integrale))\n",
    "        discount_factor = 1.0 - torch.exp(-2 * integral_beta)\n",
    "        \n",
    "        # 4. g(t)^2 = β(t) * discount_factor\n",
    "        g_squared = beta_t * discount_factor\n",
    "        \n",
    "        return g_squared\n",
    "    \n",
    "\n",
    "        \n",
    "    def get_lambda_original(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes λ(t) = (1 - exp(-∫_0^t β(s) ds))^2\n",
    "        \"\"\"\n",
    "        integral_beta = self.get_integral_beta(t)\n",
    "        # 1 - exp(-Integral) è un fattore comune nel formalismo DDPM/SDE\n",
    "        alpha_t_factor = torch.exp(-integral_beta) \n",
    "        return (1.0 - alpha_t_factor) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2981ade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "# import torch\n",
    "# from subVP_SDE import subVP_SDE\n",
    "# from Configurations import ForwardConfig\n",
    "\n",
    "class ForwardProcess:\n",
    "    def __init__(self, beta_min: float = 0.1, beta_max: float = 20.0, N: int = 1000):\n",
    "        self.beta_min = beta_min\n",
    "        self.beta_max = beta_max\n",
    "        self.N = N\n",
    "        self.sde_model = subVP_SDE(beta_min=beta_min, beta_max=beta_max, N=N)\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_noised_latents(self, z0: torch.Tensor, t: torch.Tensor, final: bool = False, eps: float = 1e-5, closed_formula : bool = True, steps: int = 500, seed: int = 42, sde_cfg: ForwardConfig = ForwardConfig()) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Return noised latents z_t, along with the exact epsilon used and std(t).\n",
    "        \n",
    "        Inputs:\n",
    "        z0: encoded latents. Device and dtype define outputs.\n",
    "        t: scalar in (0, 1). If None and final=False, defaults to 0.5 for a mid-horizon corruption level.\n",
    "        final: if True, overrides t and uses t = 1 - eps to avoid t=1 exactly for numerical stability when computing σ(t).\n",
    "        eps: small offset so that final-time evaluation uses 1 - eps instead of 1.0. Prevents sqrt(1 - exp(…)) from degenerating.\n",
    "        sde_cfg: ForwardProcess instance carrying beta schedule and N. Used to build a subVP_SDE with matching parameters.\n",
    "\n",
    "        Operations:\n",
    "        1. Builds subVP_SDE(beta_min, beta_max, N) on the same device as z0.\n",
    "        2. Broadcasts scalar t to a batch vector (B,) for the SDE call.\n",
    "        3. Calls closed-form perturbation: z_t = μ(t|z0) + σ(t) * ε, where ε ~ N(0, I) if not supplied internally by subVP_SDE.\n",
    "        4. Returns (z_t, ε, σ(t)), where σ(t) has shape (B,).\n",
    "\n",
    "        Notes:\n",
    "        - Deterministic given z0, t, and a fixed epsilon.\n",
    "        - Useful for reproducible corruption by reusing returned epsilon.\n",
    "        \"\"\"\n",
    "        if isinstance(t, torch.Tensor):\n",
    "            t_tensor = t\n",
    "\n",
    "            if not closed_formula:\n",
    "                raise ValueError(\"Training requires closed_formula=True when t is a tensor.\")\n",
    "        \n",
    "        \n",
    "        else: \n",
    "            if final:\n",
    "                t_val = 1.0 - float(eps)\n",
    "            else:\n",
    "                t_val = 0.5 if t is None else float(t)\n",
    "\n",
    "\n",
    "        # Building the SDE on the same device of the latent vector\n",
    "        # sde = subVP_SDE(beta_min=sde_cfg.beta_min, beta_max=sde_cfg.beta_max, N=sde_cfg.N)\n",
    "\n",
    "        if closed_formula:\n",
    "            #t_tensor = torch.full((z0.size(0),), t_tensor, device=z0.device, dtype=z0.dtype)\n",
    "            z_t, epsilon, std = self.sde_model.perturb_closed(z0, t_tensor)\n",
    "        else:\n",
    "            # t_tensor = torch.tensor([t_val], device=z0.device, dtype=z0.dtype)\n",
    "            z_t, epsilon, std = self.sde_model.perturb_simulate_path(z0, t_val, steps=steps, seed=seed)        \n",
    "        \n",
    "        \n",
    "        return z_t, epsilon, std\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def main():\n",
    "        cfg = ForwardConfig()\n",
    "        z0 = torch.load(cfg.input_path, map_location=\"cpu\")\n",
    "    \n",
    "        sde_cfg = ForwardProcess(cfg.beta_min,cfg.beta_max, cfg.N)\n",
    "    \n",
    "        z_t, epsilon, std = get_noised_latents(z0, t = cfg.t, final = cfg.final, eps = cfg.eps, closed_formula = cfg.closed_formula, steps = cfg.N, seed = cfg.seed, sde_cfg = sde_cfg)\n",
    "    \n",
    "        torch.save(z_t, cfg.output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c6236aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tensor_image(tensor_img, title=\"\"):\n",
    "    \"\"\"\n",
    "    Converte un tensore (C, H, W) nel range [0, 1] in un'immagine numpy e la mostra.\n",
    "    \"\"\"\n",
    "    if tensor_img.device.type != 'cpu':\n",
    "        tensor_img = tensor_img.cpu()\n",
    "        \n",
    "    # Se il tensore è un batch (B, C, H, W), prendiamo il primo elemento\n",
    "    if tensor_img.ndim == 4:\n",
    "        tensor_img = tensor_img[0]\n",
    "        \n",
    "    # Clampa per sicurezza e converte in NumPy (H, W, C)\n",
    "    img_np = tensor_img.clamp(0, 1).permute(1, 2, 0).numpy()\n",
    "    \n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(img_np)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "57f8236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- 4. FUNZIONE DI VISUALIZZAZIONE E SETUP ---\n",
    "# def setup():\n",
    "#     \"\"\"Setup del modello e loop di training.\"\"\"\n",
    "    \n",
    "#     print(f\"Inizializzazione su {DEVICE}...\")\n",
    "\n",
    "#     # A. Data Loading\n",
    "#     try:\n",
    "#         dataset = LatentDataset(data_dir=DATA_FOLDER, image_size=IMAGE_SIZE)\n",
    "#         dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "#         print(f\"Dataset caricato: {len(dataset)} immagini.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"ERRORE: Impossibile trovare o caricare i dati in {DATA_FOLDER}. Verifica il path. {e}\")\n",
    "#         return\n",
    "\n",
    "#     # Visualizzazione e Test Trasformazione\n",
    "#     example_img = dataset[0]\n",
    "#     print(f\"Immagine originale (pixel) shape: {example_img.shape}\")\n",
    "\n",
    "#     show_tensor_image(example_img, title=\"1. Immagine Originale (Pixels)\")\n",
    "\n",
    "#     # B. Model Setup\n",
    "#     model = UNet(in_channels=LATENT_CHANNELS, out_channels=LATENT_CHANNELS, features=FEATURES).to(DEVICE)\n",
    "#     diffusion = GaussianDiffusion(timesteps=TIMESTEPS)\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "#     criterion = nn.MSELoss() # La loss standard per DDPM è MSELoss tra rumore predetto e rumore reale\n",
    "\n",
    "#     print(\"--- Inizio Test VAE e UNet ---\")\n",
    "    \n",
    "#     # 1. Encoding\n",
    "#     dummy_pixels = example_img.unsqueeze(0).to(DEVICE)\n",
    "#     latents = encode_to_latent(dummy_pixels)\n",
    "#     print(f\"Latent (encoded) shape: {latents.shape}\") # Dovrebbe essere (1, 4, 16, 16) per 128x128\n",
    "    \n",
    "#     # 2. Diffusion Forward Test (t=500)\n",
    "#     test_t = torch.tensor([500]).to(DEVICE)\n",
    "#     noisy_latents, true_noise = diffusion.forward_process(latents, test_t)\n",
    "#     print(f\"Latent rumoroso (t=500) shape: {noisy_latents.shape}\")\n",
    "#     show_tensor_image(noisy_latents, title=f\"3. Immagine Rumorosa (t={test_t.item()})\")\n",
    "\n",
    "#     # 3. UNet Prediction Test\n",
    "#     predicted_noise = model(noisy_latents, test_t)\n",
    "#     print(f\"Rumore predetto (output UNet) shape: {predicted_noise.shape}\")\n",
    "#     assert predicted_noise.shape == true_noise.shape, \"Shape mismatch tra rumore predetto e reale!\"\n",
    "\n",
    "#     print(\"Test UNet OK. Inizio Training.\")\n",
    "    \n",
    "#     print(\"-\" * 30)\n",
    "\n",
    "#     return model, diffusion, dataloader, optimizer, criterion\n",
    "\n",
    "\n",
    "\n",
    "# model, diffusion, dataloader, optimizer, criterion = setup()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f2d4317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Subset #\n",
    "# NUM_SAMPLES_FOR_SANITY_CHECK = 5000 \n",
    "\n",
    "# def small_dataset():\n",
    "    \n",
    "#     # A. Data Loading\n",
    "#     try:\n",
    "#         # Inizializza il Dataset Completo (con la logica di caricamento file/classi esistente)\n",
    "#         full_dataset = LatentDataset(data_dir=DATA_FOLDER, image_size=IMAGE_SIZE)\n",
    "        \n",
    "        \n",
    "#         if len(full_dataset) > NUM_SAMPLES_FOR_SANITY_CHECK:\n",
    "#             # 1. Genera N indici casuali (da 0 alla dimensione totale del dataset)\n",
    "#             indices = torch.randperm(len(full_dataset))[:NUM_SAMPLES_FOR_SANITY_CHECK].tolist()\n",
    "            \n",
    "#             # 2. Crea il Subset utilizzando solo questi indici\n",
    "#             subset_dataset = Subset(full_dataset, indices)\n",
    "            \n",
    "#             print(f\"Dataset completo caricato ({len(full_dataset)} immagini).\")\n",
    "#             print(f\"Utilizzo Subset di {len(subset_dataset)} immagini per il Sanity Check.\")\n",
    "#         else:\n",
    "#             subset_dataset = full_dataset\n",
    "#             print(\"Dataset troppo piccolo. Utilizzo tutte le immagini.\")\n",
    "\n",
    "#         # 3. Passa il Subset (o il full_dataset) al DataLoader\n",
    "#         dataloader = DataLoader(subset_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        \n",
    "#         return dataloader\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"ERRORE: Impossibile trovare o caricare i dati in {DATA_FOLDER}. Verifica il path. {e}\")\n",
    "#         return\n",
    "\n",
    "# small_dataloader = small_dataset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "09de1ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_importance_sampling_probabilities(sde_model, N_timesteps, device):\n",
    "    \"\"\"\n",
    "    Calcola il tensore di probabilità p_IS per l'Importance Sampling.\n",
    "    p(t) ∝ g(t)^2 / λ_orig(t)\n",
    "    \"\"\"\n",
    "    T_max = 1.0\n",
    "    epsilon = 1e-8 # Per stabilità numerica (evitare divisioni per zero)\n",
    "    \n",
    "    # 1. Crea il vettore di timestep continui da [eps, 1.0]\n",
    "    timesteps = torch.linspace(epsilon, T_max, N_timesteps, device=device)\n",
    "    \n",
    "    # 2. Calcola i pesi necessari (g(t)^2 e λ_orig(t))\n",
    "    g_squared = sde_model.get_g_squared(timesteps)\n",
    "    lambda_original = sde_model.get_lambda_original(timesteps)\n",
    "    \n",
    "    # 3. Calcola il peso non normalizzato p(t) ∝ g(t)^2 / λ_orig(t)\n",
    "    # Aggiungiamo epsilon al denominatore per sicurezza.\n",
    "    sampling_weights = g_squared / (lambda_original + epsilon)\n",
    "    \n",
    "    # 4. Normalizza per ottenere la distribuzione di probabilità (somma = 1)\n",
    "    probabilities = sampling_weights / torch.sum(sampling_weights)\n",
    "    \n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "238c6e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inizializzazione su cuda...\n",
      "Dataset caricato: Totale 5000 immagini.\n",
      " -> Train Loader: 4000 immagini (125 batches)\n",
      " -> Validation Loader: 1000 immagini (32 batches)\n"
     ]
    }
   ],
   "source": [
    "# --- NUOVE COSTANTI GLOBALI NECESSARIE ---\n",
    "def setup():\n",
    "    \"\"\"Setup del modello e preparazione dei data loader di training e validazione.\"\"\"\n",
    "    \n",
    "    print(f\"Inizializzazione su {DEVICE}...\")\n",
    "\n",
    "    # A. Data Loading & Splitting\n",
    "    try:\n",
    "        # 1. Carica il Dataset Completo\n",
    "        full_dataset = LatentDataset(data_dir=DATA_FOLDER, image_size=IMAGE_SIZE)\n",
    "        \n",
    "        # 2. Definisci le dimensioni dello split\n",
    "        val_size = int(VALIDATION_SPLIT_RATIO * len(full_dataset))\n",
    "        train_size = len(full_dataset) - val_size\n",
    "        \n",
    "        # 3. SPLIT DETERMINISTICO (Usiamo un seed per la riproducibilità!)\n",
    "        torch.manual_seed(42)\n",
    "        train_dataset, val_dataset = random_split(\n",
    "            full_dataset, [train_size, val_size]\n",
    "        )\n",
    "        \n",
    "        # 4. Crea i DataLoader separati\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "        print(f\"Dataset caricato: Totale {len(full_dataset)} immagini.\")\n",
    "        print(f\" -> Train Loader: {len(train_dataset)} immagini ({len(train_loader)} batches)\")\n",
    "        print(f\" -> Validation Loader: {len(val_dataset)} immagini ({len(val_loader)} batches)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERRORE: Impossibile trovare o caricare i dati in {DATA_FOLDER}. Verifica il path. {e}\")\n",
    "        return\n",
    "\n",
    "    # B. Model Setup\n",
    "    model = UNet(in_channels=LATENT_CHANNELS, out_channels=LATENT_CHANNELS, features=FEATURES).to(DEVICE)\n",
    "    forward_process = ForwardProcess(beta_min=0.1, beta_max=20.0, N=1000)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.MSELoss() \n",
    "\n",
    "    importance_sampling_probabilities = calculate_importance_sampling_probabilities(\n",
    "    forward_process.sde_model, \n",
    "    forward_process.N, \n",
    "    DEVICE\n",
    ")\n",
    "\n",
    "    # 5. Restituisce il modello, la diffusione e I DUE DATALOADER\n",
    "    return model, train_loader, val_loader, optimizer, forward_process, criterion, importance_sampling_probabilities\n",
    "\n",
    "model, train_loader, val_loader, optimizer, forward_process, criterion, importance_sampling_probabilities = setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ea58a33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 (TRAIN): 100%|██████████| 125/125 [00:48<00:00,  2.59it/s, loss=5.3009]\n",
      "Epoch 1/5 [VAL]: 100%|██████████| 32/32 [00:11<00:00,  2.71it/s, loss=5.2453]\n",
      "Epoch 2/5 (TRAIN): 100%|██████████| 125/125 [00:44<00:00,  2.81it/s, loss=5.1260]\n",
      "Epoch 2/5 [VAL]: 100%|██████████| 32/32 [00:10<00:00,  3.09it/s, loss=5.0363]\n",
      "Epoch 3/5 (TRAIN): 100%|██████████| 125/125 [00:45<00:00,  2.74it/s, loss=5.3119]\n",
      "Epoch 3/5 [VAL]: 100%|██████████| 32/32 [00:10<00:00,  3.13it/s, loss=4.7602]\n",
      "Epoch 4/5 (TRAIN): 100%|██████████| 125/125 [00:44<00:00,  2.83it/s, loss=4.9173]\n",
      "Epoch 4/5 [VAL]: 100%|██████████| 32/32 [00:10<00:00,  3.07it/s, loss=5.3259]\n",
      "Epoch 5/5 (TRAIN): 100%|██████████| 125/125 [00:45<00:00,  2.76it/s, loss=5.2827]\n",
      "Epoch 5/5 [VAL]: 100%|██████████| 32/32 [00:10<00:00,  3.10it/s, loss=4.9270]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completato!\n",
      "Pesi e cronologia finali salvati in: ../weights/ldm_unet_final_T1000_LR00001_E5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5 \n",
    "def train(model, train_loader, val_loader, optimizer, forward_process, criterion, vae_scale_factor = 0.18215, importance_sampling_probabilities =  None, device = \"cuda\"):\n",
    "    \"\"\"\n",
    "    Esegue il loop di training DDPM, salvando i pesi e i metadati.\n",
    "    Include la validazione inline (senza funzione esterna).\n",
    "    \"\"\"\n",
    "\n",
    "    # Crea la cartella di salvataggio se non esiste\n",
    "    os.makedirs(SAVE_FOLDER, exist_ok=True)\n",
    "    \n",
    "    # Inizializza la cronologia delle loss PRIMA del ciclo delle Epoche\n",
    "    loss_history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        # 1. --- FASE DI TRAINING (Modalità: model.train()) ---\n",
    "        model.train() \n",
    "        total_train_loss = 0\n",
    "        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} (TRAIN)\")\n",
    "        \n",
    "        for step, batch in enumerate(train_bar):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Logica Forward/Backward\n",
    "            x_start_pixels = batch.to(device)\n",
    "            x_start_latents = encode_to_latent(x_start_pixels) * vae_scale_factor\n",
    "            batch_size = x_start_latents.shape[0]\n",
    "\n",
    "            if importance_sampling_probabilities is not None:\n",
    "                indices = torch.multinomial(importance_sampling_probabilities, num_samples=batch_size, replacement=True)\n",
    "                t_float_values = indices.float() / forward_process.N \n",
    "                t = t_float_values.to(device)\n",
    "            else: \n",
    "                t = torch.rand(x_start_latents.shape[0], device=device)\n",
    "\n",
    "\n",
    "            true_noise = torch.randn_like(x_start_latents)\n",
    "\n",
    "            x_t, _, std_t = forward_process.get_noised_latents(x_start_latents, t = t, eps = true_noise)           \n",
    "            \n",
    "            #x_t, true_noise = diffusion.forward_process(x_start_latents, t, noise)\n",
    "            \n",
    "            predicted_noise = model(x_t, t)\n",
    "\n",
    "\n",
    "            # Not weighted loss per sample\n",
    "            per_sample_loss = criterion(predicted_noise, true_noise)\n",
    "\n",
    "            # Weighting factor λ(t)\n",
    "            weighting_factor = forward_process.sde_model.get_g_squared(t)[:, None, None, None]\n",
    "            \n",
    "            # Total weighted loss\n",
    "            weighted_loss = per_sample_loss * weighting_factor\n",
    "            \n",
    "            loss = torch.mean(weighted_loss)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            avg_train_loss = total_train_loss / (step + 1)\n",
    "            train_bar.set_postfix(loss=f'{avg_train_loss:.4f}') # Aggiorna la barra TQDM\n",
    "            \n",
    "        final_avg_train_loss = total_train_loss / len(train_loader) \n",
    "            \n",
    "        # 2. --- FASE DI VALIDAZIONE (Dopo il ciclo di training) ---\n",
    "        \n",
    "        # Disattiva il calcolo del gradiente per la valutazione\n",
    "        with torch.no_grad():\n",
    "            model.eval() # Imposta il modello in modalità valutazione (disattiva dropout/batchnorm)\n",
    "            total_val_loss = 0\n",
    "\n",
    "            val_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [VAL]\")\n",
    "\n",
    "            for val_batch in val_bar:\n",
    "                x_start_pixels = val_batch.to(device)\n",
    "                x_start_latents = encode_to_latent(x_start_pixels) * vae_scale_factor\n",
    "\n",
    "\n",
    "                batch_size = x_start_latents.shape[0]\n",
    "\n",
    "                if importance_sampling_probabilities is not None:\n",
    "                    indices = torch.multinomial(importance_sampling_probabilities, num_samples=batch_size, replacement=True)\n",
    "                    t_float_values = indices.float() / forward_process.N \n",
    "                    t = t_float_values.to(device)\n",
    "                else: \n",
    "                    t = torch.rand(x_start_latents.shape[0], device=device)\n",
    "\n",
    "\n",
    "                true_noise = torch.randn_like(x_start_latents)\n",
    "\n",
    "\n",
    "                x_t, _, std_t = forward_process.get_noised_latents(x_start_latents, t = t, eps = true_noise)           \n",
    "                \n",
    "                \n",
    "                                \n",
    "                predicted_noise = model(x_t, t)\n",
    "\n",
    "                # Not weighted loss per sample\n",
    "                per_sample_loss = criterion(predicted_noise, true_noise)\n",
    "\n",
    "                # Weighting factor λ(t)\n",
    "                weighting_factor = forward_process.sde_model.get_g_squared(t)[:, None, None, None]\n",
    "                \n",
    "                # Total weighted loss\n",
    "                weighted_loss = per_sample_loss * weighting_factor\n",
    "            \n",
    "                loss = torch.mean(weighted_loss)\n",
    "\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                avg_val_loss = total_val_loss / (val_bar.n + 1) \n",
    "                val_bar.set_postfix(loss=f'{avg_val_loss:.4f}')\n",
    "\n",
    "            avg_val_loss = total_val_loss / len(val_loader)\n",
    "            \n",
    "        model.train() # Riporta il modello in modalità training per l'epoca successiva\n",
    "        \n",
    "        # --- LOGGING E SALVATAGGIO ---\n",
    "        \n",
    "        loss_history['train_loss'].append(final_avg_train_loss)\n",
    "        loss_history['val_loss'].append(avg_val_loss)\n",
    "\n",
    "        \n",
    "        \n",
    "        # Checkpoint: Salva lo stato completo del modello e la history\n",
    "        # checkpoint_data = {\n",
    "        #      'epoch': epoch + 1,\n",
    "        #      'model_state_dict': model.state_dict(),\n",
    "        #      'optimizer_state_dict': optimizer.state_dict(),\n",
    "        #      'avg_val_loss': avg_val_loss,\n",
    "        #      'loss_history': loss_history, \n",
    "        # }\n",
    "        \n",
    "        # checkpoint_path = os.path.join(\n",
    "        #     SAVE_FOLDER, \n",
    "        #     f\"{MODEL_NAME}_epoch_{epoch+1:03d}_val_{avg_val_loss:.4f}.pth\"\n",
    "        # )\n",
    "        # torch.save(checkpoint_data, checkpoint_path)\n",
    "        # print(f\"Checkpoint salvato in: {checkpoint_path}\")\n",
    "\n",
    "    # --- LOGICA DI SALVATAGGIO FINALE (Fuori dal ciclo epoch) ---\n",
    "    print(\"Training completato!\")\n",
    "    lr_str = str(LEARNING_RATE).replace('.', '') \n",
    "    hyper_suffix = f\"T{TIMESTEPS}_LR{lr_str}_E{EPOCHS}\"\n",
    "    final_model_filename = f\"{MODEL_NAME}_final_{hyper_suffix}.pth\"\n",
    "    final_model_path = os.path.join(SAVE_FOLDER, final_model_filename)\n",
    "\n",
    "    torch.save({'state_dict': model.state_dict()}, final_model_path)\n",
    "    \n",
    "    print(f\"Pesi e cronologia finali salvati in: {final_model_path}\")\n",
    "    return loss_history\n",
    "\n",
    "loss_history = train(model, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, forward_process=forward_process, criterion=criterion, importance_sampling_probabilities=importance_sampling_probabilities, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a100cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_loss_history(history: dict):\n",
    "    \"\"\"\n",
    "    Visualizza l'andamento delle loss di training e validazione.\n",
    "\n",
    "    Args:\n",
    "        history (dict): Dizionario contenente le liste 'train_loss' e 'val_loss'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Verifica che le chiavi necessarie siano presenti\n",
    "    if 'train_loss' not in history or 'val_loss' not in history:\n",
    "        print(\"Errore: Il dizionario history deve contenere le chiavi 'train_loss' e 'val_loss'.\")\n",
    "        return\n",
    "\n",
    "    train_losses = history['train_loss']\n",
    "    val_losses = history['val_loss']\n",
    "    \n",
    "    # Genera l'asse X (numero di epoche)\n",
    "    epochs = range(1, len(train_losses) + 1) \n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plotta la Training Loss\n",
    "    plt.plot(epochs, train_losses, 'b-o', label='Training Loss')\n",
    "    \n",
    "    # Plotta la Validation Loss\n",
    "    plt.plot(epochs, val_losses, 'r-s', label='Validation Loss')\n",
    "    \n",
    "    # Aggiunge i dettagli al grafico\n",
    "    plt.title('Andamento della Loss (DDPM)')\n",
    "    plt.xlabel('Epoca')\n",
    "    plt.ylabel('Loss Media (MSE)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    # Forza i tick dell'asse X a essere interi (utile se hai poche epoche)\n",
    "    plt.xticks(epochs) \n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# --- Esempio di Utilizzo ---\n",
    "\n",
    "# 1. Esegui il training e ottieni la history\n",
    "# loss_history = train(model, diffusion, train_loader, val_loader, optimizer, criterion)\n",
    "# print(\"Training completato. Generazione del grafico...\")\n",
    "\n",
    "# 2. Chiama la funzione di plot\n",
    "# plot_loss_history(loss_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
